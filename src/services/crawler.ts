import * as cheerio from 'cheerio';
import { loadConfig } from '../utils/config';

export interface CrawlerOptions {
  /**
   * Fetch implementation (e.g., global fetch or mocked fetch in tests)
   */
  fetchFn: (url: string) => Promise<{ text: () => Promise<string> }>;

  /**
   * CSS selector used to locate post links on list pages
   */
  postLinkSelector: string;
}

export interface Crawler {
  /**
   * Discover all post URLs by crawling list pages with pagination.
   */
  discoverPostUrls: () => Promise<string[]>;
}

/**
 * Creates a Crawler service that discovers post URLs from a Tistory blog.
 *
 * Pagination strategy:
 * - Start from the base blog URL (treated as page=1).
 * - Next pages are generated by incrementing the `page` query parameter:
 *   - page 1: `BLOG_URL`
 *   - page 2: `BLOG_URL?page=2`
 *   - page 3: `BLOG_URL?page=3`, etc.
 * - If the next page does not exist (fetch fails), crawling stops.
 *
 * This focuses on URL discovery and pagination only. Individual post fetching
 * and metadata parsing are handled by separate tasks (T021, T022).
 */
export const createCrawler = (options: CrawlerOptions): Crawler => {
  const config = loadConfig();
  const { fetchFn, postLinkSelector } = options;

  const resolveUrl = (path: string): string => {
    if (path.startsWith('http://') || path.startsWith('https://')) {
      return path;
    }

    return `${config.blogUrl.replace(/\/$/, '')}${path.startsWith('/') ? '' : '/'}${path}`;
  };

  const buildPageUrl = (page: number): string => {
    if (page === 1) {
      return config.blogUrl;
    }

    const separator = config.blogUrl.includes('?') ? '&' : '?';
    return `${config.blogUrl}${separator}page=${page}`;
  };

  const fetchPage = async (url: string): Promise<{ html: string }> => {
    const response = await fetchFn(url);
    const html = await response.text();
    return { html };
  };

  const extractPostUrls = (html: string): string[] => {
    const $ = cheerio.load(html);
    const urls: string[] = [];

    $(postLinkSelector).each((_, element) => {
      const href = $(element).attr('href');
      if (href) {
        urls.push(resolveUrl(href));
      }
    });

    return urls;
  };

  const discoverPostUrls = async (): Promise<string[]> => {
    const discoveredUrls = new Set<string>();
    let page = 1;

    // Keep requesting sequential pages until a page is missing or empty.
    // - Missing page: fetch throws (network/404) -> stop.
    // - Empty page: no post links found -> stop.
    // This prevents infinite crawling and matches the requested behavior.
    //
    // 페이지 전략:
    // - 1페이지: BLOG_URL (page=1 취급)
    // - 다음 페이지: 현재 page + 1, 즉 ?page=2, ?page=3 ...
    // - 다음 페이지가 없거나(post가 없거나 fetch 실패) 하면 탐색 종료
    //
    while (true) {
      const pageUrl = buildPageUrl(page);

      let html: string;
      try {
        const result = await fetchPage(pageUrl);
        html = result.html;
      } catch {
        // 다음 페이지가 존재하지 않는 경우
        break;
      }

      const pagePostUrls = extractPostUrls(html);

      if (pagePostUrls.length === 0) {
        // 더 이상 post URL이 없는 경우
        break;
      }

      for (const url of pagePostUrls) {
        if (!discoveredUrls.has(url)) {
          discoveredUrls.add(url);
        }
      }

      page += 1;
    }

    return Array.from(discoveredUrls);
  };

  return {
    discoverPostUrls,
  };
};
